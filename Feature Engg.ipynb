{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a7037f-0715-46f1-8f90-cf25e62f995c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\\napplication'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f195e9-155a-4ae6-b0e7-28881ffac9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Min-Max Scaling:\\n\\nDefinition: Min-Max scaling, also known as normalization, is a data preprocessing technique that rescales features to a fixed range,\\ntypically [0, 1]. It transforms each feature to a specific range by subtracting the minimum value and dividing by the range of the feature.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Min-Max Scaling:\n",
    "\n",
    "Definition: Min-Max scaling, also known as normalization, is a data preprocessing technique that rescales features to a fixed range,\n",
    "typically [0, 1]. It transforms each feature to a specific range by subtracting the minimum value and dividing by the range of the feature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137a2b1c-5cec-4cb3-b4b2-859e98aebaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "Feature: [50, 60, 70, 80, 90]\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example data\n",
    "data = np.array([[50], [60], [70], [80], [90]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b538c8-2d1c-43e0-a725-6f8e33760525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\\nProvide an example to illustrate its application.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e839b344-deb7-43a1-9124-9201742c55f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Definition: Unit vector scaling, also known as normalization to unit length, rescales features so that the feature vector has a length of 1. \\nThis is done by dividing each feature value by the Euclidean norm (L2 norm) of the vector.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Definition: Unit vector scaling, also known as normalization to unit length, rescales features so that the feature vector has a length of 1. \n",
    "This is done by dividing each feature value by the Euclidean norm (L2 norm) of the vector.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630d7ad6-3c09-48c4-af7a-4dec33b941f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 0.8]]\n"
     ]
    }
   ],
   "source": [
    "Feature : [3, 4]\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Example data\n",
    "data = np.array([[3, 4]])\n",
    "\n",
    "# Normalize data to unit vector\n",
    "normalized_data = normalize(data, norm='l2')\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f568f2-f8d0-4d2b-96f8-d8056abfb70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\\nexample to illustrate its application'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d44a302-c348-45cb-a6d6-7bc8dac83a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal Component Analysis (PCA):\\n\\nDefinition: PCA is a dimensionality reduction technique that transforms data into a new coordinate system where the greatest variances by any projection of the data come to lie on the first coordinates called principal components. It reduces the number of dimensions while retaining most of the variance (information) in the data.\\n\\nUsage in Dimensionality Reduction:\\n\\nReduce Complexity: By projecting data onto fewer dimensions, PCA simplifies the data while retaining essential patterns.\\nImprove Performance: It helps in speeding up machine learning algorithms and reducing overfitting by eliminating redundant features.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Principal Component Analysis (PCA):\n",
    "\n",
    "Definition: PCA is a dimensionality reduction technique that transforms data into a new coordinate system where the greatest variances by any projection of the data come to lie on the first coordinates called principal components. It reduces the number of dimensions while retaining most of the variance (information) in the data.\n",
    "\n",
    "Usage in Dimensionality Reduction:\n",
    "\n",
    "Reduce Complexity: By projecting data onto fewer dimensions, PCA simplifies the data while retaining essential patterns.\n",
    "Improve Performance: It helps in speeding up machine learning algorithms and reducing overfitting by eliminating redundant features.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d353f2-323c-49ae-be15-186d61c966a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.60994863 -0.11396371]\n",
      " [ 1.05085985  0.126715  ]\n",
      " [-0.08470482  0.06123228]\n",
      " [-1.22026949 -0.00425043]\n",
      " [-2.35583416 -0.06973314]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data: 3 features\n",
    "data = np.array([[2, 3, 4],\n",
    "                 [3, 6, 9],\n",
    "                 [4, 8, 12],\n",
    "                 [5, 10, 15],\n",
    "                 [6, 12, 18]])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "print(data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf61a72-6e41-44bd-bb85-a3f45637e949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\\nExtraction? Provide an example to illustrate this concept'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eef032b-b591-4a38-9ab0-13d99f5fc617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relationship between PCA and Feature Extraction:\\n\\nPCA (Principal Component Analysis) is a specific technique used for feature extraction. Feature extraction involves transforming original features into a new set of features that are more informative or compact. PCA performs this transformation by creating new features (principal components) that capture the most variance in the data.\\n\\nUsing PCA for Feature Extraction:\\n\\nExtract Principal Components: PCA finds the principal components (new features) that capture the maximum variance in the data.\\nDimensionality Reduction: By selecting the top principal components, you reduce the number of features while retaining the most important information'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Relationship between PCA and Feature Extraction:\n",
    "\n",
    "PCA (Principal Component Analysis) is a specific technique used for feature extraction. Feature extraction involves transforming original features into a new set of features that are more informative or compact. PCA performs this transformation by creating new features (principal components) that capture the most variance in the data.\n",
    "\n",
    "Using PCA for Feature Extraction:\n",
    "\n",
    "Extract Principal Components: PCA finds the principal components (new features) that capture the maximum variance in the data.\n",
    "Dimensionality Reduction: By selecting the top principal components, you reduce the number of features while retaining the most important information'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122873d6-3380-4da1-8dc9-1a667ad91d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.42425314  0.09433097]\n",
      " [ 0.69092637 -0.12384746]\n",
      " [-0.30404696 -0.05944936]\n",
      " [-1.47862934 -0.05622134]\n",
      " [-2.33250321  0.14518718]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data with 4 features\n",
    "data = np.array([[2, 3, 4, 5],\n",
    "                 [4, 6, 8, 10],\n",
    "                 [5, 7, 9, 12],\n",
    "                 [6, 8, 11, 14],\n",
    "                 [7, 9, 12, 15]])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA to reduce to 2 features\n",
    "pca = PCA(n_components=2)\n",
    "data_reduced = pca.fit_transform(data_standardized)\n",
    "print(data_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e26fa066-ef7d-4127-baf6-827996f6b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\\ncontains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\\npreprocess the data.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f956f321-8f96-4172-afaf-b5568dffac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price    rating  delivery_time\n",
      "0   0.00  0.000000           1.00\n",
      "1   0.25  0.333333           0.75\n",
      "2   0.50  0.666667           0.50\n",
      "3   0.75  1.000000           0.25\n",
      "4   1.00  0.866667           0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\n",
    "    'price': [10, 20, 30, 40, 50],\n",
    "    'rating': [3.5, 4.0, 4.5, 5.0, 4.8],\n",
    "    'delivery_time': [30, 25, 20, 15, 10]\n",
    "})\n",
    "\n",
    "# Create MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56104d6-2903-47f3-874c-7bc38560e0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\\nfeatures, such as company financial data and market trends. Explain how you would use PCA to reduce the\\ndimensionality of the dataset.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d4a74d-6aff-4773-96a6-7e56fbc76783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2\n",
      "0  2.770029 -0.058398\n",
      "1 -0.648709 -2.062922\n",
      "2 -0.000000  0.000000\n",
      "3 -1.520596 -0.106382\n",
      "4 -0.600724  2.227703\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\n",
    "    'revenue': [10, 20, 30, 40, 50],\n",
    "    'profit': [1, 2, 3, 4, 5],\n",
    "    'market_index': [1000, 1100, 1050, 1080, 1020],\n",
    "    'trading_volume': [500, 600, 550, 580, 520]\n",
    "})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(data_pca.shape[1])])\n",
    "print(pca_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "793dae12-6dcd-412a-a96c-9abddc8a3ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\\nvalues to a range of -1 to 1.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a5c4ea-26cf-4dbb-8b6f-177599941e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min-Max scaling to [0, 1]\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "scaled_data = (data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Transform to [-1, 1]\n",
    "final_data = 2 * scaled_data - 1\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "443093cb-f262-4ba8-ae3d-88d8df9467a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\\nFeature Extraction using PCA. How many principal components would you choose to retain, and why?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f5cefc2-c14b-4a10-aac9-e0a3856bf7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data with features: height, weight, age, gender (encoded), blood pressure\n",
    "data = pd.DataFrame({\n",
    "    'height': [160, 170, 180, 190, 200],\n",
    "    'weight': [55, 70, 85, 90, 95],\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'gender': [0, 1, 0, 1, 0],  # Example encoding: 0 for female, 1 for male\n",
    "    'blood_pressure': [120, 130, 140, 150, 160]\n",
    "})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(data_standardized)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Number of components to retain\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1  # Retain enough components to explain 95% variance\n",
    "print(f'Number of principal components to retain: {num_components}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a811512-a5bb-494c-8b23-3e294e55072d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
